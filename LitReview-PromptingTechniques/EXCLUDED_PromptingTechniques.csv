Title,Authors,Technique,Link,EC_1:Not_prompt_engineering,EC_2:Not_Text_Code_LLMs,EC_3:Cant_Use_for_Code_Generation,EC_4:Automatic_Prompting_Techniques_OR_Frameworks,EC_5:Prompt_Engineering_Attacks,EC_6:Others,Detailed Exclusion Reason
Active Prompting with Chain-of-Thought for Large Language Models,Diao et al.,Active prompt,https://arxiv.org/abs/2302.12246,x,,,x,,,"The focus of this work is on the selection of best task-specific examples for the LLM. Active prompting is a way to improve the performance of CoT technique by using a more effective (uncertainity-based selection strategy) method to select the best questions or examples to annotate. Since the novelty of the technique is in the selection of few-shot CoT examples provided to the model, we decided to exclude this as a new prompting technique (EC_1). Additionally, to adapt this technique for code generation tasks, one will have to use additional scripts, tools or frameworks to identify unique solutions given by the LLM to calculate the uncertainity metric (since code responses, as opposed to reasoning task responses, are much more complicated to use simple string comparisons to identify similar or unique responses) (EC_4). "
Ask me anything: A simple strategy for prompting language models,Arora et al.,Ask Me Anything,https://arxiv.org/abs/2210.02441,,,x,,,,"AMA is a technique that tries to get accurate answers by transforming restrictive questions (such as yes/no or true/false or cloze-style) into open-ended questions. However, since code generation tasks are inherently open-ended, we excluded this technique from our list (EC_3)"
A recipe for arbitrary text style transfer with large language models,Reif et al.,Augmented zero-shot,https://arxiv.org/abs/2109.03910,,,x,,,,"This technique is designed to change the style of a given text (e.g., more descriptive, more positive, more comic). Since this is not suitable for code generation, this technique was exlcuded (EC_3)."
Large language models are human-level prompt engineers,Zhou et al.,Automatic Prompt Engineer,https://arxiv.org/abs/2211.01910,,,,x,,,"APE finds optimal prompts by generating multiple instruction options for a given task by directly inferring or recursively deriving them based on semantic similarity. These candidate instructions are then executed using the target model, and the most suitable one is chosen based on computed evaluation scores. Since this is an automated framework rather than a technique that involves directly prompting the LLM we excluded this (EC_4)"
ART: Automatic multi-step reasoning and tool-use for large language models,Paranjape et al.,Automatic Reasoning and Tool-use,https://arxiv.org/abs/2303.09014,,,,x,,,"ART is another framework that automatically generates step-by-step decompositions, also known as multi-step reasoning, for new task instances. It also involves the usage of external tools such as search engines, task libraries, program grammars, optional feedback and so on. Since this is an automated approach as opposed to the other prompting techniques considered in our study, it requires a different experimental setup for evalaution, and hence was excluded from our selection (EC_4)"
Guiding Large Language Models via Directional Stimulus Prompting,Li et al.,Directional Stimulus Prompting,https://arxiv.org/abs/2302.11520,,,,x,,,It is an automated framework where they use an external model such as T5 as the policy model to generate the directional stimulus prompt for each input query. It requires training the policy model through supervised fine-tuning (SFT) using a few collected labeled data which is outside of this study's scope (EC_4).
Generated Knowledge Prompting for Commonsense Reasoning,Liu et al.,Generated knowledge prompting,https://arxiv.org/abs/2110.08387,,,,x,,,Uses two different LLM in its pipeline one for knowledge generation and one for knowledge integration/inference. Out of scope (EC_4)
GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks,Liu et al. ,Graph Prompt,https://arxiv.org/abs/2302.08043,,,x,,,,Prompting framework for graphs (EC_3)
Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations,Jung et al. ,Maieutic Prompting,https://arxiv.org/abs/2205.11822,,,x,x,,,"Maieutic prompting induces the LM to generate abductive explanations for diverse hypotheses with deep recursive reasoning, then collectively eliminates the contradicting candidates, resulting in consistent answers. It is more appropriate for fact checking and true or false questions. It also requires some automation for depthwise knowledge spanning (EC_3, EC_4)"
Multimodal Chain-of-Thought Reasoning in Language Models,Zhang et al.,Multi-modal CoT,https://arxiv.org/abs/2302.00923,,x,,,,,"Focuses on providing multi-modal information to the LLM to generate better results. For this they have proposed a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. For code generation we only focus on the language features, hence this technique is irrelvant for our usecase (EC_2)"
Large Language Models as Optimizers,Yang et al.,OPRO,https://arxiv.org/abs/2309.03409,,,,x,,,"This technique works by optimizing prompts. The prompt to the LLM serves as a call to the optimizer, called the meta-prompt. The meta-prompt contains two core pieces of information. The first piece is previously generated prompts with their corresponding training accuracies. The second piece is the optimization problem description, which includes several exemplars randomly selected from the training set to exemplify the task of interest. This is an automated approach which out of our scope (EC_4). More suitable for optimization problems."
Promptsource: An integrated development environment and repository for natural language prompts,Bach et al.,Prompt Source,https://arxiv.org/abs/2202.01279,x,,,,,,"It is an integrated development environment and a repository for prompts for zero-shot learning. It is not a prompting technique but rather an open-source system for creating, sharing, and using natural language prompts and addresses the need for new collaborative and centralized tools to support the emerging research around prompting."
ReAct: Synergizing Reasoning and Acting in Language Models,Yao et al.,ReAct,https://arxiv.org/abs/2210.03629,,,x,,,,"A pardigm that combines reasoning and acting with language models for solving diverse language reasoning and decision making tasks. It enables the llms to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason). More suitable for tasks with with distinct action spaces and reasoning needs, including QA, fact verification, text game, and web navigation. Unclear how to use it for code generation, will have to perform dedicated study to adapt this technique for code generation."
Reflexion: Language Agents with Verbal Reinforcement Learning,Shinn et al.,Reflexion,https://arxiv.org/abs/2303.11366,,,,x,,,"They propose language agents that use verbal reinforcements to learn from prior failings. This approach uses 3 distinct models - actor model that generates text and actions, evaluator model that scores the output produced by the actor and a self-reflection models that provides reinforcement cues to help the actor improves its generations. Even though this can be used for code generation, more than a prompting technique it is a prompting framework that is out of our scope. (EC_4)"
Dialogue state tracking with a language model using schema-driven prompting,Lee et al. ,Schema driven prompting,https://arxiv.org/abs/2109.07506,,,x,,,,Approach used for dialog history tracking to infer the user goal. Not a direct prompting technique and also not relevant for code generation tasks (EC_3)
Selection-inference: Exploiting large language models for interpretable logical reasoning,Creswell et al.,Selection inference,https://arxiv.org/abs/2205.09712,,,x,,,,Its a framework where multiple steps of selection and inference are chained together to produce a sequence of reasoning steps. In the selection they select a subset of information present in the context and the inference step produces a fact based on the selection. For this approach they require a context filled with facts and rules. Additionally this approach is mentioned to be suitale for questions that have definitive answers. Hence it is exlcuded (EC_3).
Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Yao et al.,Tree of thoughts,https://arxiv.org/abs/2305.10601,,,,x,,,Automated framework that uses search heuristic algorithms such as BFS and DFS to generate and optimize the the prompts that lead better solutions. Out of scope according to EC_4. 
Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks,Chen et al.,Program-of-Thoughts,https://arxiv.org/abs/2211.12588,x,,,,,,"PoT tries to enhance the accuracy of the LLMs in solving reasoning tasks such as arithmetic or math problems by making the LLM generate code to solve the problem instead of having the LLM directly return the final answer. The generated code is then given to an external language interpreter that executes the program and returns the answer. Since our use case is code generation itself, the novelty introduced by this technique becomes irrelevant transforming this technique to CoT prompting."