Title,Authors,Technique,Link,EC_1:Not_prompt_engineering,EC_2:Not_Text_Code_LLMs,EC_3:Cant_Use_for_Code_Generation,EC_4:Automatic_Prompting_Techniques,EC_5:Prompt_Engineering_Attacks,EC_6:Others,Detailed Exclusion Reason
Active Prompting with Chain-of-Thought for Large Language Models,"Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang",Active prompt,https://arxiv.org/abs/2302.12246,x,,,x,,,"The focus of this work is on the selection of best task-specific examples for the LLM. Active prompting is a way to improve the performance of CoT technique by using a more effective (uncertainity-based selection strategy) method to select the best questions or examples to annotate. Since the novelty of the technique is in the selection of few-shot CoT examples provided to the model, we decided to exclude this as a new prompting technique (EC_1). Additionally, to adapt this technique for code generation tasks, one will have to use additional scripts, tools or frameworks to identify unique solutions given by the LLM to calculate the uncertainity metric (since code responses, as opposed to reasoning task responses, are much more complicated to use simple string comparisons to identify similar or unique responses) (EC_4). "
Ask me anything: A simple strategy for prompting language models,"Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, Christopher Ré",Ask Me Anything,https://arxiv.org/abs/2210.02441,,,x,,,,"AMA is a technique that tries to get accurate answers by transforming restrictive questions (such as yes/no or true/false or cloze-style) into open-ended questions. However, since code generation tasks are inherently open-ended, we excluded this technique from our list (EC_3)"
A recipe for arbitrary text style transfer with large language models,"Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, Jason Wei",Augmented zero-shot,https://arxiv.org/abs/2109.03910,,,x,,,,"This technique is designed to change the style of a given text (e.g., more descriptive, more positive, more comic). Since this is not suitable for code generation, this technique was exlcuded (EC_3)."
Large language models are human-level prompt engineers,"Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba",Automatic Prompt Engineer,https://arxiv.org/abs/2211.01910,,,,x,,,"APE finds optimal prompts by generating multiple instruction options for a given task by directly inferring or recursively deriving them based on semantic similarity. These candidate instructions are then executed using the target model, and the most suitable one is chosen based on computed evaluation scores. Since this is an automated framework rather than a technique that involves directly prompting the LLM we excluded this (EC_4)"
ART: Automatic multi-step reasoning and tool-use for large language models,"Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio Ribeiro",Automatic Reasoning and Tool-use,https://arxiv.org/abs/2303.09014,,,,x,,,"ART is another framework that automatically generates step-by-step decompositions, also known as multi-step reasoning, for new task instances. It also involves the usage of external tools such as search engines, task libraries, program grammars, optional feedback and so on. Since this is an automated approach as opposed to the other prompting techniques considered in our study, it requires a different experimental setup for evalaution, and hence was excluded from our selection (EC_4)"
Complexity-Based Prompting for Multi-Step Reasoning,"Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot",Complexity based prompting,https://arxiv.org/abs/2210.00720,x,,,,,,"Complexity-based prompting is very similar to the self-consistency technique that is included in the final list of techniques. Just like self-consistency, complexity-based prompting follows CoT prompting and generates multiple reasoning chains that lead to the final answer. The main difference is that this technique focuses on selecting examples with larger number of reasoning steps during prompting. Additionally instead of voting among all N reasoning chains returned by the LLM to get the final answer, this technique only votes among top K (K ≤ N) complex reasoning chains with larger number of reasoning steps. Hence we only identify this technique as a variation of self-consistency technique leading to its exclusion using EC_1."
Guiding Large Language Models via Directional Stimulus Prompting,"Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan",Directional Stimulus Prompting,https://arxiv.org/abs/2302.11520,,,,x,,,It is an automated framework where they use an external model such as T5 as the policy model to generate the directional stimulus prompt for each input query. It requires training the policy model through supervised fine-tuning (SFT) using a few collected labeled data which is outside of this study's scope (EC_4).
Generated Knowledge Prompting for Commonsense Reasoning,"Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, Hannaneh Hajishirzi",Generated knowledge prompting,https://arxiv.org/abs/2110.08387,,,,x,,,Uses two different LLM in its pipeline one for knowledge generation and one for knowledge integration/inference. Out of scope (EC_4)
GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks,"Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang",Graph Prompt,https://arxiv.org/abs/2302.08043,,,x,,,,Prompting framework for graphs (EC_3)
Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations,"Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi",Maieutic Prompting,https://arxiv.org/abs/2205.11822,,,x,x,,,"Maieutic prompting induces the LM to generate abductive explanations for diverse hypotheses with deep recursive reasoning, then collectively eliminates the contradicting candidates, resulting in consistent answers. It is more appropriate for fact checking and true or false questions. It also requires some automation for depthwise knowledge spanning (EC_3, EC_4)"
Multimodal Chain-of-Thought Reasoning in Language Models,"Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola",Multi-modal CoT,https://arxiv.org/abs/2302.00923,,x,,,,,"Focuses on providing multi-modal information to the LLM to generate better results. For this they have proposed a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. For code generation we only focus on the language features, hence this technique is irrelvant for our usecase (EC_2)"
Large Language Models as Optimizers,"Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen",OPRO,https://arxiv.org/abs/2309.03409,,,,x,,,"This technique works by optimizing prompts. The prompt to the LLM serves as a call to the optimizer, called the meta-prompt. The meta-prompt contains two core pieces of information. The first piece is previously generated prompts with their corresponding training accuracies. The second piece is the optimization problem description, which includes several exemplars randomly selected from the training set to exemplify the task of interest. This is an automated approach which out of our scope (EC_4). More suitable for optimization problems."
Promptsource: An integrated development environment and repository for natural language prompts,"Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-Jian Jiang, Alexander M. Rush",Prompt Source,https://arxiv.org/abs/2202.01279,x,,,,,,"It is an integrated development environment and a repository for prompts for zero-shot learning. It is not a prompting technique but rather an open-source system for creating, sharing, and using natural language prompts and addresses the need for new collaborative and centralized tools to support the emerging research around prompting."
ReAct: Synergizing Reasoning and Acting in Language Models,"Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao",ReAct,https://arxiv.org/abs/2210.03629,,,x,,,,"A pardigm that combines reasoning and acting with language models for solving diverse language reasoning and decision making tasks. It enables the llms to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason). More suitable for tasks with with distinct action spaces and reasoning needs, including QA, fact verification, text game, and web navigation. Unclear how to use it for code generation, will have to perform dedicated study to adapt this technique for code generation."
Reflexion: Language Agents with Verbal Reinforcement Learning,"Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao",Reflexion,https://arxiv.org/abs/2303.11366,,,,x,,,"They propose language agents that use verbal reinforcements to learn from prior failings. This approach uses 3 distinct models - actor model that generates text and actions, evaluator model that scores the output produced by the actor and a self-reflection models that provides reinforcement cues to help the actor improves its generations. Even though this can be used for code generation, more than a prompting technique it is a prompting framework that is out of our scope. (EC_4)"
Dialogue state tracking with a language model using schema-driven prompting,"Chia-Hsuan Lee, Hao Cheng, Mari Ostendorf",Schema driven prompting,https://arxiv.org/abs/2109.07506,,,x,,,,Approach used for dialog history tracking to infer the user goal. Not a direct prompting technique and also not relevant for code generation tasks (EC_3)
Selection-inference: Exploiting large language models for interpretable logical reasoning,"Antonia Creswell, Murray Shanahan, Irina Higgins",Selection inference,https://arxiv.org/abs/2205.09712,,,x,,,,Its a framework where multiple steps of selection and inference are chained together to produce a sequence of reasoning steps. In the selection they select a subset of information present in the context and the inference step produces a fact based on the selection. For this approach they require a context filled with facts and rules. Additionally this approach is mentioned to be suitale for questions that have definitive answers. Hence it is exlcuded (EC_3).
Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan",Tree of thoughts,https://arxiv.org/abs/2305.10601,,,,x,,,Automated framework that uses search heuristic algorithms such as BFS and DFS to generate and optimize the the prompts that lead better solutions. Out of scope according to EC_4. 
Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks,"Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen",Program-of-Thoughts,https://arxiv.org/abs/2211.12588,x,,,,,,"PoT tries to enhance the accuracy of the LLMs in solving reasoning tasks such as arithmetic or math problems by making the LLM generate code to solve the problem instead of having the LLM directly return the final answer. The generated code is then given to an external language interpreter that executes the program and returns the answer. Since our use case is code generation itself, the novelty introduced by this technique becomes irrelevant transforming this technique to CoT prompting."